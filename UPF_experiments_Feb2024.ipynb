{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPF Expeiments\n",
    "    Feb2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (401682, 239)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, \n",
    "                             recall_score, roc_auc_score, mean_squared_error, r2_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from config import region_mapping, set1, to_model_a, to_model_c, to_model_ac, to_model, set10\n",
    "\n",
    "\n",
    "path = '/Users/jerzybala/Desktop/Simulation Experiments Aug-Sep 2023/PROCESSED_FOOD_Dec2023.csv'\n",
    "\n",
    "path_temp = '/Volumes/2TB Ext/BrainBaseMHQ/mhm_data_2024-02-03_17-00-42_from jan2023.csv'\n",
    "\n",
    "path_temp_processed = '/Volumes/2TB Ext/BrainBaseMHQ/GMP_data_2023_to_jan2024.csv'\n",
    "\n",
    "df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.loc[:, 'Processed food in diet'] = df['Processed food in diet'].replace({\n",
    "            'Rarely/never': 'Rarely/Never',\n",
    "            'A few times in a day': 'Several times a day',\n",
    "            'Several days a week': 'A few times a week',\n",
    "            'Many times in a day': 'Several times a day',\n",
    "            'At least once a day': 'Several times a day'\n",
    "        })\n",
    "\n",
    "\n",
    "print(f\"Data loaded. Shape: {df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_features = df[set10].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_prepare_df(df, countries=None, age_groups=None, drop_list=None):\n",
    "    \"\"\"\n",
    "    Segments the DataFrame based on specific countries and age groups, and then drops specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to be processed.\n",
    "    - countries: List of countries for segmentation. If None, no country-based segmentation is applied.\n",
    "    - age_groups: List of age groups for segmentation. If None, no age-based segmentation is applied.\n",
    "    - drop_list: List of columns to be dropped from the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame that has been segmented and had specified columns dropped.\n",
    "    \"\"\"\n",
    "    # Segment DataFrame based on provided countries and age groups\n",
    "    segmented_df = df.copy()\n",
    "    if countries is not None:\n",
    "        segmented_df = segmented_df[segmented_df['Country'].isin(countries)]\n",
    "    if age_groups is not None:\n",
    "        segmented_df = segmented_df[segmented_df['Age'].isin(age_groups)]\n",
    "    \n",
    "    # Drop specified columns if drop_list is provided\n",
    "    if drop_list is not None:\n",
    "        segmented_df = segmented_df.drop(columns=drop_list)\n",
    "    \n",
    "    return segmented_df\n",
    "#================================================================================================\n",
    "\n",
    "\n",
    "def encode_features(df, categorical_features):\n",
    "    \"\"\"Encodes categorical features using one-hot encoding.\"\"\"\n",
    "    encoded_features = pd.get_dummies(df[categorical_features])\n",
    "    return pd.concat([df.drop(columns=categorical_features), encoded_features], axis=1)\n",
    "#================================================================================================\n",
    "\n",
    "\n",
    "def prepare_dataset(df, target_classification, target_regression):\n",
    "    \"\"\"Prepares the dataset by dropping specified columns, encoding categorical features, and splitting into features and targets.\"\"\"\n",
    "    #df_filtered = drop_columns(df, drop_list)\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns\n",
    "    df_filtered = encode_features(df, categorical_features)\n",
    "    \n",
    "    X = df_filtered.drop(columns=[target_classification, target_regression], axis=1)\n",
    "    y_classification = df_filtered[target_classification]\n",
    "    y_regression = df_filtered[target_regression]\n",
    "    \n",
    "    return X, y_classification, y_regression\n",
    "\n",
    "#================================================================================================\n",
    "\n",
    "def split_dataset(X, y_classification, y_regression, test_size=0.3, random_state=42):\n",
    "    \"\"\"Splits the dataset into training and testing sets for both classification and regression tasks.\"\"\"\n",
    "    X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=test_size, random_state=random_state)\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train_class, y_test_class, X_train_reg, X_test_reg, y_train_reg, y_test_reg\n",
    "\n",
    "#================================================================================================\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39375, 46)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = [\n",
    "\n",
    "    'Country',\n",
    "\n",
    "    'Household Income',\n",
    "\n",
    "    'ARCHIVED: Smartphone ownership',\n",
    "    'ARCHIVED: Age of smartphone access',\n",
    "    'Smartphone allowed in school',\n",
    "    'Smartphone use in lessons',\n",
    "\n",
    "    'Smartphone ownership',\n",
    "    'Friends/classmates smarphone ownership',\n",
    "    'Age of smartphone usage during school hours',\n",
    "    'Smartphone usage during class hours',\n",
    "    'Smartphone usage during break'\n",
    "\n",
    "]\n",
    "\n",
    "anglosphere = ['United States', 'United Kingdom', 'Canada', 'Australia', 'New Zealand', 'Ireland']\n",
    "kraj = ['United States']\n",
    "kraj2 =['India']\n",
    "wiek = ['18-24', '21-24', '18', '19', '20']\n",
    "wiek2 = ['55-64','45-54']\n",
    "\n",
    "\n",
    "# Segment and prepare the DataFrame\n",
    "prepared_df = segment_and_prepare_df(df_features, countries=anglosphere, age_groups=None, drop_list=drop_list)\n",
    "\n",
    "\n",
    "prepared_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepared_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Legal_Substance_Use = [\n",
    "    'Tobacco products',\n",
    "    'Alcoholic beverages',\n",
    "    'Cannabis',\n",
    "    'Vaping products',\n",
    "    'Sedatives or Sleeping Pills',\n",
    "    'Amphetamine type stimulants (e.g. speed| diet pills| ecstasy| etc.)',\n",
    "    'Opioids',\n",
    "    'Melatonina',\n",
    "    'Cigarrillo'\n",
    "]\n",
    "\n",
    "Interpersonal_Trauma = [\n",
    "    'Prolonged emotional or psychological abuse or neglect from parent/caregiver',\n",
    "    'Physical violence in the home between family members',\n",
    "    'Prolonged or sustained bullying in person from peers',\n",
    "    'Prolonged physical abuse| or severe physical assault CT',\n",
    "    'Threatening| coercive or controlling behavior by another person CT',\n",
    "    'Threatening| coercive or controlling behavior by another person',\n",
    "    'Prolonged sexual abuse| or severe sexual assault.',\n",
    "    'Cyberbullying or online abuse',\n",
    "    'Forced family control over major life decisions (e.g. marriage)',\n",
    "    'Forced family control over major life decisions CT'\n",
    "]\n",
    "\n",
    "Life_Adversities = [\n",
    "    'Sudden or premature death of a loved one',\n",
    "    'Parental Divorce or family breakup',\n",
    "    'Divorce/separation  or family breakup',\n",
    "    'Extreme poverty leading to homelessness and/or hunger.',\n",
    "    'Sudden or premature death of a parent or sibling',\n",
    "    'Loss of your job or livelihood leading to an inability to make ends meet.',\n",
    "    'Lived with a parent/caregiver who was an alcoholic or who regularly used street drugs',\n",
    "    'Life threatening or debilitating injury or illness.',\n",
    "    'Suffered a loss in a major fire| flood| earthquake| or natural disaster',\n",
    "    'Displacement from your home due to political| environmental or economic reasons CT',\n",
    "    'Suffered a loss in a major fire| flood| earthquake| or natural disaster CT',\n",
    "    'Life threatening or debilitating injury or illness CT',\n",
    "    'Displacement from your home due to political| environmental or economic reasons',\n",
    "    'Involvement or close witness to a war',\n",
    "    'Caring for a child or partner with a major chronic disability or illness'\n",
    "]\n",
    "\n",
    "\n",
    "prepared_df['Legal_Substance_Use'] = prepared_df[Legal_Substance_Use].sum(axis=1)\n",
    "prepared_df['Interpersonal_Trauma'] = prepared_df[Interpersonal_Trauma].sum(axis=1)\n",
    "prepared_df['Life_Adversities'] = prepared_df[Life_Adversities].sum(axis=1)\n",
    "\n",
    "\n",
    "prepared_df.to_csv('/Users/jerzybala/Desktop/prepared_df.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Exploreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include or not to include UPF\n",
    "\n",
    "prepared_df=prepared_df.drop('Processed food in diet', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_infogain(df, target):\n",
    "    # Calculate the entropy of the target variable\n",
    "    target_entropy = entropy(df[target].value_counts(normalize=True), base=2)\n",
    "    \n",
    "    # Calculate the entropy of each feature\n",
    "    feature_entropies = []\n",
    "    for column in df.columns:\n",
    "        if column != target:\n",
    "            feature_entropy = 0\n",
    "            for value in df[column].unique():\n",
    "                subset = df[df[column] == value]\n",
    "                subset_entropy = entropy(subset[target].value_counts(normalize=True), base=2)\n",
    "                feature_entropy += (len(subset) / len(df)) * subset_entropy\n",
    "            feature_entropies.append(feature_entropy)\n",
    "    \n",
    "    # Calculate the information gain for each feature\n",
    "    infogains = target_entropy - np.array(feature_entropies)\n",
    "    \n",
    "    # Create a dictionary mapping each feature to its information gain\n",
    "    infogain_dict = dict(zip(df.columns[df.columns != target], infogains))\n",
    "    \n",
    "    return infogain_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df_c =prepared_df.drop(columns='Overall MHQ', axis=1)\n",
    "\n",
    "\n",
    "info_gained_features = calculate_infogain(prepared_df_c, 'MHQ_Sign')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_sorted_feature_scores(feature_scores, score_name=\"Score\", table_format=\"grid\"):\n",
    "    \"\"\"\n",
    "    Sorts a dictionary of features and their scores in descending order and prints it in tabulated format.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_scores: Dictionary with features as keys and scores as values.\n",
    "    - score_name: A string representing the name of the score for the header. Defaults to \"Score\".\n",
    "    - table_format: A string representing the table format to be used by tabulate. Defaults to \"grid\".\n",
    "    \"\"\"\n",
    "    # Convert the dictionary to a list of tuples and sort it in descending order by score\n",
    "    feature_scores_sorted = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the sorted list in tabulate format with customizable headers\n",
    "    print(tabulate(feature_scores_sorted, headers=[\"Feature\", score_name], tablefmt=table_format))\n",
    "\n",
    "# Example usage for information gain:\n",
    "print_sorted_feature_scores(info_gained_features, score_name=\"Information Gain\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to Excel\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "features_ranks = pd.DataFrame(list(info_gained_features.items()), columns=['Feature', 'Information Gain'])\n",
    "\n",
    "# Sort the DataFrame by 'Information Gain' in descending order\n",
    "features_ranks_sorted = features_ranks.sort_values(by='Information Gain', ascending=False)\n",
    "\n",
    "# Specify the file path (adjust the path as necessary for your system)\n",
    "file_path = '/Users/jerzybala/Desktop/sapien_temp.xlsx'\n",
    "\n",
    "# Write the sorted DataFrame to an Excel file\n",
    "features_ranks_sorted.to_excel(file_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Feature scores have been written to {file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File deletion\n",
    "\n",
    "#  CAREFUL! This will delete the file at the specified path\n",
    "# ******************************************************\n",
    "\n",
    "# After ensuring the file is no longer needed, delete it\n",
    "os.remove(file_path)\n",
    "print(f\"The file {file_path} has been deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list1 = ['Interpersonal_Trauma', 'Age', 'Legal_Substance_Use', 'Life_Adversities']\n",
    "\n",
    "\n",
    "for l in list1:\n",
    "    a = prepared_df[l].unique().tolist()\n",
    "    print(f\"{l}: {a}, number of unique={len(a)}\")   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df.to_csv('/Users/jerzybala/Desktop/prepared_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y_classification, y_regression = prepare_dataset(prepared_df, 'MHQ_Sign', 'Overall MHQ')\n",
    "\n",
    "X_train, X_test, y_train, y_test, X_train_reg, X_test_reg, y_train_reg, y_test_reg = split_dataset(\n",
    "    X, \n",
    "    y_classification, \n",
    "    y_regression, \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb.XGBClassifier:\n",
      "Accuracy: 0.8214678743756878\n",
      "F1 Score: 0.8932638291411509\n",
      "Precision: 0.8363343442001516\n",
      "Recall: 0.9585098294775715\n",
      "AUC: 0.8322790244100866\n",
      "\n",
      "\n",
      "xgb.XGBRegressor:\n",
      "mae: 44.94170743862004\n",
      "rmse: 56.44146975569511\n",
      "r2: 0.40346480687535413\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "par1 = {\n",
    "    'n_estimators': 200, \n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 10, \n",
    "    'min_child_weight': 1, \n",
    "    'gamma': 0.01, \n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier and train\n",
    "\n",
    "model_C = xgb.XGBClassifier(**par1)\n",
    "model_C.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = model_C.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "auc = roc_auc_score(y_test, model_C.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(\"xgb.XGBClassifier:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "#==================================================================================================\n",
    "\n",
    "\n",
    "# Initialize XGBoost regressor and train\n",
    "#model = xgb.XGBRegressor()\n",
    "# X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "#model_R = xgb.XGBRegressor()\n",
    "\n",
    "model_R = xgb.XGBRegressor(\n",
    "n_estimators=500,\n",
    "learning_rate=0.1,\n",
    "max_depth=6,\n",
    "min_child_weight=1,\n",
    "gamma=0.2,\n",
    "subsample=0.4,\n",
    "colsample_bytree=0.8,\n",
    "#reg_alpha=10,\n",
    "reg_lambda=0.1\n",
    ")\n",
    "\n",
    "model_R.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Separate predictions based on the sign of y_test\n",
    "predictions = model_R.predict(X_test_reg)\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, predictions)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, predictions)\n",
    "mae = mean_absolute_error(y_test_reg, predictions)\n",
    "\n",
    "print(\"xgb.XGBRegressor:\")\n",
    "print('mae:', mae)\n",
    "print('rmse:',rmse)\n",
    "print('r2:', r2)\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_prepare_df(df, countries=None, age_groups=None, drop_list=None):\n",
    "    \"\"\"\n",
    "    Segments the DataFrame based on specific countries and age groups, and then drops specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to be processed.\n",
    "    - countries: List of countries for segmentation. If None, no country-based segmentation is applied.\n",
    "    - age_groups: List of age groups for segmentation. If None, no age-based segmentation is applied.\n",
    "    - drop_list: List of columns to be dropped from the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame that has been segmented and had specified columns dropped.\n",
    "    \"\"\"\n",
    "    # Segment DataFrame based on provided countries and age groups\n",
    "    segmented_df = df.copy()\n",
    "    if countries is not None:\n",
    "        segmented_df = segmented_df[segmented_df['Country'].isin(countries)]\n",
    "    if age_groups is not None:\n",
    "        segmented_df = segmented_df[segmented_df['Age'].isin(age_groups)]\n",
    "    \n",
    "    # Drop specified columns if drop_list is provided\n",
    "    if drop_list is not None:\n",
    "        segmented_df = segmented_df.drop(columns=drop_list)\n",
    "    \n",
    "    return segmented_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your initial dataframe\n",
    "countries = ['United States', 'United Kingdom', 'Canada', 'Australia', 'New Zealand', 'Ireland', 'South Africa']\n",
    "age_groups = ['18-24', '21-24', '18', '19', '20']\n",
    "drop_list = [\n",
    "    'Country',\n",
    "    'Smartphone ownership',\n",
    "    'Friends/classmates smartphone ownership',\n",
    "    'Age of smartphone usage during school hours',\n",
    "    'Smartphone usage during class hours',\n",
    "    'Smartphone usage during break'\n",
    "]\n",
    "\n",
    "# Segment and prepare the DataFrame\n",
    "prepared_df = segment_and_prepare_df(df, countries=countries, age_groups=age_groups, drop_list=drop_list)\n",
    "\n",
    "# Continue with further data preparation steps as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, drop_list, target_classification, target_regression):\n",
    "    \"\"\"Prepares the dataset by dropping specified columns, encoding categorical features, and splitting into features and targets.\"\"\"\n",
    "    df_filtered = drop_columns(df, drop_list)\n",
    "    categorical_features = df_filtered.select_dtypes(include=['object']).columns\n",
    "    df_filtered = encode_features(df_filtered, categorical_features)\n",
    "    \n",
    "    X = df_filtered.drop(columns=[target_classification, target_regression], axis=1)\n",
    "    y_classification = df_filtered[target_classification]\n",
    "    y_regression = df_filtered[target_regression]\n",
    "    \n",
    "    return X, y_classification, y_regression\n",
    "\n",
    "def split_dataset(X, y_classification, y_regression, test_size=0.3, random_state=42):\n",
    "    \"\"\"Splits the dataset into training and testing sets for both classification and regression tasks.\"\"\"\n",
    "    X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=test_size, random_state=random_state)\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train_class, y_test_class, X_train_reg, X_test_reg, y_train_reg, y_test_reg\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use these functions\n",
    "drop_list = [\n",
    "    'Country',\n",
    "    'Smartphone ownership',\n",
    "    'Friends/classmates smartphone ownership',\n",
    "    'Age of smartphone usage during school hours',\n",
    "    'Smartphone usage during class hours',\n",
    "    'Smartphone usage during break'\n",
    "]\n",
    "\n",
    "# Assuming df is your initial dataframe\n",
    "X, y_classification, y_regression = prepare_dataset(df, drop_list, 'MHQ_Sign', 'Overall MHQ')\n",
    "X_train, X_test, y_train_class, y_test_class, X_train_reg, X_test_reg, y_train_reg, y_test_reg = split_dataset(X, y_classification, y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_processed = df[set10].copy()\n",
    "\n",
    "df_processed.loc[:, 'Processed food in diet'] = df_processed['Processed food in diet'].replace({\n",
    "            'Rarely/never': 'Rarely/Never',\n",
    "            'A few times in a day': 'Several times a day',\n",
    "            'Several days a week': 'A few times a week',\n",
    "            'Many times in a day': 'Several times a day',\n",
    "            'At least once a day': 'Several times a day',\n",
    "            'None of the above': 'None of the above SU'\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filter the encoded dataframe\n",
    "anglosphere = ['United States', 'United Kingdom', 'Canada', 'Australia', 'New Zealand', 'Ireland']\n",
    "\n",
    "\n",
    "kraj = ['United States']\n",
    "kraj2 =['India']\n",
    "        \n",
    "\n",
    "wiek = ['18-24', '21-24', '18', '19', '20']\n",
    "wiek2 = ['55-64','45-54']\n",
    "\n",
    "\n",
    "#df_filtered = df_processed[(df_processed['Country'].isin(kraj)) & (df_processed['Age'].isin(wiek))]\n",
    "\n",
    "\n",
    "#(2)\n",
    "#df_filtered = df_processed[(df_processed['Country'].isin(anglosphere))]\n",
    "\n",
    "#(1)\n",
    "#df_filtered = df_processed.copy()\n",
    "\n",
    "\n",
    "#(3) \n",
    "#df_filtered = df_processed[(df_processed['Country'].isin(kraj))]\n",
    "\n",
    "\n",
    "#4 & 5#\n",
    "#df_filtered = df_processed[(df_processed['Country'].isin(kraj)) & (df_processed['Age'].isin(wiek))]\n",
    "\n",
    "\n",
    "\n",
    "#df_filtered = df_processed[(df_processed['Age'].isin(wiek2))]\n",
    "\n",
    "\n",
    "\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
