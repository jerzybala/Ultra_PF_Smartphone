{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPF Expeiments\n",
    "    Feb2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, \n",
    "                             recall_score, roc_auc_score, mean_squared_error, r2_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from config import region_mapping, set1, to_model_a, to_model_c, to_model_ac, to_model, set10\n",
    "\n",
    "path = '/Users/jerzybala/Desktop/Simulation Experiments Aug-Sep 2023/PROCESSED_FOOD_Dec2023.csv'\n",
    "path_temp = '/Volumes/2TB Ext/BrainBaseMHQ/mhm_data_2024-02-03_17-00-42_from jan2023.csv'\n",
    "path_temp_processed = '/Volumes/2TB Ext/BrainBaseMHQ/GMP_data_2023_to_jan2024.csv'\n",
    "path1 = '/Users/jerzybala/Desktop/PROCESSED_FOOD_Dec2023.csv'\n",
    "\n",
    "df = pd.read_csv(path1, low_memory=False)\n",
    "\n",
    "df.loc[:, 'Processed food in diet'] = df['Processed food in diet'].replace({\n",
    "            'Rarely/never': 'Rarely/Never',\n",
    "            'A few times in a day': 'Several times a day',\n",
    "            'Several days a week': 'A few times a week',\n",
    "            'Many times in a day': 'Several times a day',\n",
    "            'At least once a day': 'Several times a day'\n",
    "        })\n",
    "\n",
    "\n",
    "print(f\"Data loaded. Shape: {df.shape}\")\n",
    "\n",
    "df_org=df.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'].value_counts().head(10).plot(kind='bar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df['MHQ_Sign'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('MHQ Sign')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bar Plot of MHQ Sign')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df['Overall MHQ'].plot.density()\n",
    "plt.xlabel('Overall MHQ')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Overall MHQ')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency feature numerical codding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "frequency_features = [\n",
    "'Frequency of getting a good nights sleep',\n",
    "'Frequency of doing exercise',\n",
    "'Processed food in diet',\n",
    "'Frequency of Socializing']\n",
    "\n",
    "for f in frequency_features:\n",
    "    print(f\"{f}: {df[f].unique()}\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.loc[:, 'Frequency of getting a good nights sleep'] = df['Frequency of getting a good nights sleep'].replace({\n",
    "    'Some of the time': 3,  # Occasional good sleep might be better than \"Most days\" depending on interpretation\n",
    "    'Most of the time': 1,  # Most frequent good sleep\n",
    "    'All of the time': 0,   # Always getting a good night's sleep\n",
    "    'Hardly ever': 4,       # Rarely getting a good night's sleep\n",
    "    'Most days': 2          # Assuming \"Most days\" is similar to \"Some of the time\", but could be adjusted based on interpretation\n",
    "})\n",
    "\n",
    "\n",
    "df.loc[:, 'Frequency of doing exercise'] = df['Frequency of doing exercise'].replace({\n",
    "    'Less than once a week': 5,\n",
    "    'Rarely/Never': 6,\n",
    "    'Few days a week': 3,\n",
    "    'Once a week': 4,\n",
    "    'Every day': 1,\n",
    "    'Some days of the week': 2, # Assuming this is less than \"Every day\" but more than \"Few days a week\"\n",
    "    'Several days a week': 3    # Similar to \"Few days a week\"\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "df.loc[:, 'Processed food in diet'] = df['Processed food in diet'].replace({\n",
    "    'A few times a week': 3,\n",
    "    'Rarely/Never': 1,\n",
    "    'A few times a month': 2,\n",
    "    'Several times a day': 5,\n",
    "    'Once a day': 4\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "df.loc[:, 'Frequency of Socializing'] = df['Frequency of Socializing'].replace({ \n",
    "    '1-3 times a month': 2,\n",
    "    'Rarely/Never': 4,\n",
    "    'Once a week': 3,\n",
    "    'Several days a week': 1\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.loc[:, 'Age'] = df['Age'].replace({ \n",
    "   \n",
    " '18':18,\n",
    " '18-24':21,\n",
    " '19':19,\n",
    " '20':20,\n",
    " '21-24':22,\n",
    " '25-34':30,\n",
    " '35-44':40,\n",
    " '45-54':50,\n",
    " '55-64':60,\n",
    " '65-74':70,\n",
    " '75-84':80,\n",
    " '85+':85\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the following code to create a new dataframe with only the features of interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_prepare_df(df, countries=None, age_groups=None, drop_list=None):\n",
    "    \"\"\"\n",
    "    Segments the DataFrame based on specific countries and age groups, and then drops specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to be processed.\n",
    "    - countries: List of countries for segmentation. If None, no country-based segmentation is applied.\n",
    "    - age_groups: List of age groups for segmentation. If None, no age-based segmentation is applied.\n",
    "    - drop_list: List of columns to be dropped from the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame that has been segmented and had specified columns dropped.\n",
    "    \"\"\"\n",
    "    # Segment DataFrame based on provided countries and age groups\n",
    "    segmented_df = df.copy()\n",
    "    if countries is not None:\n",
    "        segmented_df = segmented_df[segmented_df['Country'].isin(countries)]\n",
    "    if age_groups is not None:\n",
    "        segmented_df = segmented_df[segmented_df['Age'].isin(age_groups)]\n",
    "    \n",
    "    # Drop specified columns if drop_list is provided\n",
    "    if drop_list is not None:\n",
    "        segmented_df = segmented_df.drop(columns=drop_list)\n",
    "    \n",
    "    return segmented_df\n",
    "#================================================================================================\n",
    "\n",
    "\n",
    "def encode_features(df, categorical_features):\n",
    "    \"\"\"Encodes categorical features using one-hot encoding.\"\"\"\n",
    "    encoded_features = pd.get_dummies(df[categorical_features])\n",
    "    return pd.concat([df.drop(columns=categorical_features), encoded_features], axis=1)\n",
    "#================================================================================================\n",
    "\n",
    "\n",
    "def prepare_dataset(df, target_classification, target_regression):\n",
    "    \"\"\"Prepares the dataset by dropping specified columns, encoding categorical features, and splitting into features and targets.\"\"\"\n",
    "    #df_filtered = drop_columns(df, drop_list)\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns\n",
    "    df_filtered = encode_features(df, categorical_features)\n",
    "    \n",
    "    X = df_filtered.drop(columns=[target_classification, target_regression], axis=1)\n",
    "    y_classification = df_filtered[target_classification]\n",
    "    y_regression = df_filtered[target_regression]\n",
    "    \n",
    "    return X, y_classification, y_regression\n",
    "\n",
    "#================================================================================================\n",
    "\n",
    "def split_dataset(X, y_classification, y_regression, test_size=0.3, random_state=42):\n",
    "    \"\"\"Splits the dataset into training and testing sets for both classification and regression tasks.\"\"\"\n",
    "    X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=test_size, random_state=random_state)\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train_class, y_test_class, X_train_reg, X_test_reg, y_train_reg, y_test_reg\n",
    "\n",
    "#================================================================================================\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data with feature subset and constructing new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_features = df[set10].copy()\n",
    "\n",
    "\n",
    "drop_list = [\n",
    "\n",
    "    'Country',\n",
    "\n",
    "    'Household Income',\n",
    "    # 'Education',\n",
    "    # 'Employment',\n",
    "    # 'Language',\n",
    "\n",
    "    'ARCHIVED: Smartphone ownership',\n",
    "    'ARCHIVED: Age of smartphone access',\n",
    "    'Smartphone allowed in school',\n",
    "    'Smartphone use in lessons',\n",
    "\n",
    "    'Smartphone ownership',\n",
    "    'Friends/classmates smarphone ownership',\n",
    "    'Age of smartphone usage during school hours',\n",
    "    'Smartphone usage during class hours',\n",
    "    'Smartphone usage during break'\n",
    "\n",
    "]\n",
    "\n",
    "anglosphere = ['United States', 'United Kingdom', 'Canada', 'Australia', 'New Zealand', 'Ireland']\n",
    "kraj = ['United States']\n",
    "kraj2 =['India']\n",
    "kraj3 = ['Egypt']\n",
    "kraj4 = ['Mexico']\n",
    "kraj5 = ['United Kingdom']\n",
    "\n",
    "\n",
    "wiek = ['18-24', '21-24', '18', '19', '20']\n",
    "wiek2 = ['55-64','45-54']\n",
    "wiek3=[22, 18, 19, 20, 21]\n",
    "\n",
    "\n",
    "# Segment and prepare the DataFrame\n",
    "prepared_df = segment_and_prepare_df(df_features, countries=None, age_groups=None, drop_list=drop_list)\n",
    "\n",
    "\n",
    "# Features construcrs defined in config.py\n",
    "# 'Legal_Substance_Use'\n",
    "# 'Interpersonal_Trauma'\n",
    "# 'Life_Adversities'\n",
    "\n",
    "\n",
    "from config import Legal_Substance_Use, Interpersonal_Trauma, Life_Adversities\n",
    "\n",
    "\n",
    "prepared_df['Legal_Substance_Use'] = prepared_df[Legal_Substance_Use].sum(axis=1)\n",
    "prepared_df['Interpersonal_Trauma'] = prepared_df[Interpersonal_Trauma].sum(axis=1)\n",
    "prepared_df['Life_Adversities'] = prepared_df[Life_Adversities].sum(axis=1)\n",
    "\n",
    "\n",
    "prepared_df.to_csv('/Users/jerzybala/Desktop/prepared_df.csv', index=False)\n",
    "\n",
    "\n",
    "prepared_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS SWITCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MHQ_Sign'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('MHQ Sign')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bar Plot of MHQ Sign: ORIGINAL')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "prepared_df['MHQ_Sign'] = prepared_df['MHQ_Sign'].replace({1: 0, 0: 1})\n",
    "\n",
    "prepared_df['MHQ_Sign'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('MHQ Sign')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bar Plot of MHQ Sign')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include or not to include UPF\n",
    "\n",
    "prepared_df=prepared_df.drop('Processed food in diet', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_classification, y_regression = prepare_dataset(prepared_df, 'MHQ_Sign', 'Overall MHQ')\n",
    "\n",
    "age_values = [\n",
    " 'Age_18',\n",
    " 'Age_18-24',\n",
    " 'Age_19',\n",
    " 'Age_20',\n",
    " 'Age_21-24',\n",
    " 'Age_25-34',\n",
    " 'Age_35-44',\n",
    " 'Age_45-54',\n",
    " 'Age_55-64',\n",
    " 'Age_65-74',\n",
    " 'Age_75-84',\n",
    " 'Age_85+']\n",
    "\n",
    "\n",
    "#X = X.drop(columns=age_values, axis=1)   # if decided that 'Age' is not a feature\n",
    "\n",
    "#X.columns.to_list()\n",
    "\n",
    "X_train, X_test, y_train, y_test, X_train_reg, X_test_reg, y_train_reg, y_test_reg = split_dataset(\n",
    "    X, \n",
    "    y_classification, \n",
    "    y_regression, \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_test_org = X_test # a copy to use later for X_test modification\n",
    "\n",
    "prepared_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute only if UPF is numerical !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 'A few times a week': 3,\n",
    "# 'Rarely/Never': 1,\n",
    "# 'A few times a month': 2,\n",
    "# 'Several times a day': 5,\n",
    "# 'Once a day': 4\n",
    "\n",
    "X_test = X_test_org\n",
    "\n",
    "values_to_change = [2, 3, 4, 5]\n",
    "replacement_value = 1\n",
    "percentage_change = 0.0\n",
    "\n",
    "\n",
    "def test_model_with_modified_X_test(X_test, y_test, values_to_change, replacement_value, percentage_change):\n",
    "    # Make sure percentage_change is between 0 and 1\n",
    "    if not 0 <= percentage_change <= 1:\n",
    "        raise ValueError(\"percentage_change must be between 0 and 1.\")\n",
    "    \n",
    "    # Make sure replacement value is not in values to change\n",
    "    if replacement_value in values_to_change:\n",
    "        raise ValueError(\"replacement_value should not be in values_to_change.\")\n",
    "\n",
    "    # Make a copy of X_test to avoid changing the original data\n",
    "    X_test_modified = X_test.copy()\n",
    "\n",
    "    # Iterate over the specified values to be changed\n",
    "    for value in values_to_change:\n",
    "        # Find the indices where 'Processed food in diet' equals the current value\n",
    "        indices = X_test_modified[X_test_modified['Processed food in diet'] == value].index\n",
    "        # Calculate how many entries to replace\n",
    "        num_entries_to_replace = int(len(indices) * percentage_change)\n",
    "        # Randomly select entries to replace\n",
    "        indices_to_replace = np.random.choice(indices, size=num_entries_to_replace, replace=False)\n",
    "        # Replace the selected entries with the replacement value\n",
    "        X_test_modified.loc[indices_to_replace, 'Processed food in diet'] = replacement_value\n",
    "        \n",
    "    \n",
    "    return X_test_modified\n",
    "\n",
    "X_test = X_test_org\n",
    "X_test = test_model_with_modified_X_test(X_test, y_test, values_to_change, replacement_value, 0.5)\n",
    "X_test['Processed food in diet'].value_counts()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# X_test = X_test_org\n",
    "# X_test = test_model_with_modified_X_test(X_test, y_test, values_to_change, replacement_value, 1.0)\n",
    "# X_test['Processed food in diet'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "par1 = {\n",
    "    'n_estimators': 200, \n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 10, \n",
    "    'min_child_weight': 1, \n",
    "    'gamma': 0.01, \n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier and train\n",
    "\n",
    "model_C = xgb.XGBClassifier(**par1)\n",
    "model_C.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = model_C.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "auc = roc_auc_score(y_test, model_C.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(\"xgb.XGBClassifier:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# # Assuming 'predictions' and 'y_test' are your predicted and actual labels\n",
    "# # Calculate precision, recall, and F1 score for class \"1\"\n",
    "# precision_class_1 = precision_score(y_test, predictions, pos_label=1)\n",
    "# recall_class_1 = recall_score(y_test, predictions, pos_label=1)\n",
    "# f1_class_1 = f1_score(y_test, predictions, pos_label=1)\n",
    "\n",
    "# print(f\"Precision for class '1': {precision_class_1}\")\n",
    "# print(f\"Recall for class '1': {recall_class_1}\")\n",
    "# print(f\"F1 Score for class '1': {f1_class_1}\")\n",
    "# print(\"\\n\")\n",
    "# print(\"\\n\")\n",
    "\n",
    "# precision_class_0 = precision_score(y_test, predictions, pos_label=0)\n",
    "# recall_class_0 = recall_score(y_test, predictions, pos_label=0)\n",
    "# f1_class_0 = f1_score(y_test, predictions, pos_label=0)\n",
    "\n",
    "# print(f\"Precision for class '0': {precision_class_0}\")\n",
    "# print(f\"Recall for class '0': {recall_class_0}\")\n",
    "# print(f\"F1 Score for class '0': {f1_class_0}\")\n",
    "# print(\"\\n\")\n",
    "# print(\"\\n\")\n",
    "\n",
    "\n",
    "# Step 1: Get predicted probabilities for the positive class (class \"1\")\n",
    "probabilities = model_C.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 2: Apply custom threshold to determine class predictions\n",
    "# Let's say your custom threshold is 0.6\n",
    "threshold = 0.6\n",
    "custom_predictions = np.where(probabilities > threshold, 1, 0)\n",
    "\n",
    "# Step 3: Compute metrics with custom predictions\n",
    "accuracy_custom = accuracy_score(y_test, custom_predictions)\n",
    "f1_custom = f1_score(y_test, custom_predictions)\n",
    "precision_custom = precision_score(y_test, custom_predictions)\n",
    "recall_custom = recall_score(y_test, custom_predictions)\n",
    "# AUC can remain the same because it evaluates model performance across all thresholds\n",
    "auc = roc_auc_score(y_test, probabilities)\n",
    "\n",
    "print(\"Metrics with Custom Threshold:\",threshold)\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"F1 Score: {f1_custom}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"AUC: {auc}\")\n",
    "# print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(accuracy_custom)\n",
    "print(f1_custom)\n",
    "print(precision_custom)\n",
    "print(recall_custom)\n",
    "print(auc)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Assuming predictions is your array of predictions from model_C.predict(X_test)\n",
    "\n",
    "# Count occurrences of 0 and 1\n",
    "counts = np.bincount(predictions)\n",
    "\n",
    "# Assuming your classes are 0 and 1, as typical in binary classification\n",
    "num_zeros = counts[0] if 0 in predictions else 0\n",
    "num_ones = counts[1] if 1 in predictions else 0\n",
    "\n",
    "print(f\"Number of 0s classified: {num_zeros}\")\n",
    "print(f\"Number of 1s classified: {num_ones}\")\n",
    "\n",
    "\n",
    "print(\"**************************************************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#==================================================================================================\n",
    "\n",
    "\n",
    "# Initialize XGBoost regressor and train\n",
    "#model = xgb.XGBRegressor()\n",
    "# X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.6, random_state=42)\n",
    "\n",
    "\n",
    "#model_R = xgb.XGBRegressor()\n",
    "\n",
    "model_R = xgb.XGBRegressor(\n",
    "n_estimators=200,\n",
    "learning_rate=0.1,\n",
    "max_depth=6,\n",
    "min_child_weight=1,\n",
    "gamma=0.2,\n",
    "subsample=0.4,\n",
    "colsample_bytree=0.8,\n",
    "#reg_alpha=10,\n",
    "reg_lambda=0.1\n",
    ")\n",
    "\n",
    "model_R.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Separate predictions based on the sign of y_test\n",
    "predictions = model_R.predict(X_test_reg)\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, predictions)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, predictions)\n",
    "mae = mean_absolute_error(y_test_reg, predictions)\n",
    "\n",
    "print(\"xgb.XGBRegressor:\")\n",
    "print('mae:', mae)\n",
    "print('rmse:',rmse)\n",
    "print('r2:', r2)\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "scenario = 'Global'\n",
    "\n",
    "\n",
    "# Create a figure with a specific size\n",
    "fig, ax = plt.subplots(figsize=(8, 10))  # Width, height in inches\n",
    "\n",
    "# Plot feature importance for the classifier, using the ax parameter\n",
    "xgb.plot_importance(model_C, ax=ax,importance_type='gain', max_num_features=50, show_values=False)\n",
    "plt.title(\"Feature Importance for XGBoost\", fontsize=16)\n",
    "\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('F Score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Create a figure with a specific size\n",
    "fig, ax = plt.subplots(figsize=(8, 10))  # Width, height in inches\n",
    "\n",
    "# Plot feature importance for the classifier, using the ax parameter\n",
    "xgb.plot_importance(model_R, ax=ax,importance_type='gain', max_num_features=50, show_values=False)\n",
    "plt.title(\"Feature Importance for XGBRegressor\", fontsize=16)\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('F Score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 1: Extract feature importance scores\n",
    "feature_importance_C = model_C.get_booster().get_score(importance_type='gain')\n",
    "feature_importance_R = model_R.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "# Step 2: Convert to DataFrame\n",
    "df_importance_C = pd.DataFrame(list(feature_importance_C.items()), columns=['Feature', 'Importance'])\n",
    "df_importance_R = pd.DataFrame(list(feature_importance_R.items()), columns=['Feature', 'Importance'])\n",
    "\n",
    "# Step 3: Sort the DataFrame\n",
    "df_importance_C_sorted = df_importance_C.sort_values(by='Importance', ascending=False)\n",
    "df_importance_R_sorted = df_importance_R.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# If you need to limit to the top 30 features, you can do so here\n",
    "df_importance_C_top30 = df_importance_C_sorted.head(10)\n",
    "df_importance_R_top30 = df_importance_R_sorted.head(10)\n",
    "\n",
    "# Assuming df_importance_C and df_importance_R contain all features and their importance\n",
    "\n",
    "# Write to Excel with pandas ExcelWriter for all features\n",
    "with pd.ExcelWriter('/Users/jerzybala/Desktop/feature_importance_models_all.xlsx') as writer:\n",
    "    df_importance_C_sorted.to_excel(writer, sheet_name='Classification_All', index=False)\n",
    "    df_importance_R_sorted.to_excel(writer, sheet_name='Regression_All', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance_C_top30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = X_test_org\n",
    "X_test = test_model_with_modified_X_test(X_test, y_test, values_to_change, replacement_value, 0.6)\n",
    "\n",
    "# Obtain predicted probabilities\n",
    "probabilities = model_C.predict_proba(X_test)\n",
    "\n",
    "# Select the probabilities of being class 1 (second column)\n",
    "prob_class_1 = probabilities[:, 1]\n",
    "\n",
    "# Define your new threshold\n",
    "threshold = 0.64 # Adjust this threshold as needed\n",
    "\n",
    "# Apply threshold to classify as 1 or 0\n",
    "predictions_with_new_threshold = (prob_class_1 >= threshold).astype(int)\n",
    "\n",
    "# Now, count the occurrences of 0 and 1 with the new threshold\n",
    "counts = np.bincount(predictions_with_new_threshold)\n",
    "\n",
    "num_zeros = counts[0]\n",
    "num_ones = counts[1]\n",
    "\n",
    "print(f\"Number of 0s classified with threshold {threshold}: {num_zeros}\")\n",
    "print(f\"Number of 1s classified with threshold {threshold}: {num_ones}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Exploreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_infogain(df, target):\n",
    "    # Calculate the entropy of the target variable\n",
    "    target_entropy = entropy(df[target].value_counts(normalize=True), base=2)\n",
    "    \n",
    "    # Calculate the entropy of each feature\n",
    "    feature_entropies = []\n",
    "    for column in df.columns:\n",
    "        if column != target:\n",
    "            feature_entropy = 0\n",
    "            for value in df[column].unique():\n",
    "                subset = df[df[column] == value]\n",
    "                subset_entropy = entropy(subset[target].value_counts(normalize=True), base=2)\n",
    "                feature_entropy += (len(subset) / len(df)) * subset_entropy\n",
    "            feature_entropies.append(feature_entropy)\n",
    "    \n",
    "    # Calculate the information gain for each feature\n",
    "    infogains = target_entropy - np.array(feature_entropies)\n",
    "    \n",
    "    # Create a dictionary mapping each feature to its information gain\n",
    "    infogain_dict = dict(zip(df.columns[df.columns != target], infogains))\n",
    "    \n",
    "    return infogain_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df_c =prepared_df.drop(columns='Overall MHQ', axis=1)\n",
    "\n",
    "\n",
    "info_gained_features = calculate_infogain(prepared_df_c, 'MHQ_Sign')\n",
    "\n",
    "X, y_classification\n",
    "\n",
    "Xy = pd.concat([X, y_classification], axis=1)\n",
    "\n",
    "info_gained_features = calculate_infogain(Xy, 'MHQ_Sign')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_sorted_feature_scores(feature_scores, score_name=\"Score\", table_format=\"grid\"):\n",
    "    \"\"\"\n",
    "    Sorts a dictionary of features and their scores in descending order and prints it in tabulated format.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_scores: Dictionary with features as keys and scores as values.\n",
    "    - score_name: A string representing the name of the score for the header. Defaults to \"Score\".\n",
    "    - table_format: A string representing the table format to be used by tabulate. Defaults to \"grid\".\n",
    "    \"\"\"\n",
    "    # Convert the dictionary to a list of tuples and sort it in descending order by score\n",
    "    feature_scores_sorted = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the sorted list in tabulate format with customizable headers\n",
    "    print(tabulate(feature_scores_sorted, headers=[\"Feature\", score_name], tablefmt=table_format))\n",
    "\n",
    "# Example usage for information gain:\n",
    "print_sorted_feature_scores(info_gained_features, score_name=\"Information Gain\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to Excel\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "features_ranks = pd.DataFrame(list(info_gained_features.items()), columns=['Feature', 'Information Gain'])\n",
    "\n",
    "# Sort the DataFrame by 'Information Gain' in descending order\n",
    "features_ranks_sorted = features_ranks.sort_values(by='Information Gain', ascending=False)\n",
    "\n",
    "# Specify the file path (adjust the path as necessary for your system)\n",
    "file_path = '/Users/jerzybala/Desktop/sapien_temp.xlsx'\n",
    "\n",
    "# Write the sorted DataFrame to an Excel file\n",
    "features_ranks_sorted.to_excel(file_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Feature scores have been written to {file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File deletion\n",
    "\n",
    "#  CAREFUL! This will delete the file at the specified path\n",
    "# ******************************************************\n",
    "\n",
    "# After ensuring the file is no longer needed, delete it\n",
    "os.remove(file_path)\n",
    "print(f\"The file {file_path} has been deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list1 = ['Interpersonal_Trauma', 'Age', 'Legal_Substance_Use', 'Life_Adversities']\n",
    "\n",
    "\n",
    "for l in list1:\n",
    "    a = prepared_df[l].unique().tolist()\n",
    "    print(f\"{l}: {a}, number of unique={len(a)}\")   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Optional, run if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df.to_csv('/Users/jerzybala/Desktop/prepared_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df['MHQ_Sign'] = prepared_df['MHQ_Sign'].replace({1: 0, 0: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "value_counts = prepared_df['MHQ_Sign'].value_counts()\n",
    "plt.bar(value_counts.index.astype(int), value_counts.values)\n",
    "plt.xticks([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer_C = shap.Explainer(model_C)\n",
    "shap_values_C = explainer_C(X_train)\n",
    "\n",
    "explainer_R = shap.Explainer(model_R)\n",
    "shap_values_R = explainer_R(X_train_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Save shap_values_C_anglo to file\n",
    "np.save('/Users/jerzybala/Desktop/shap_values_C.npy', shap_values_C)\n",
    "\n",
    "# Save shap_values_R_anglo to file\n",
    "np.save('/Users/jerzybala/Desktop/shap_values_R.npy', shap_values_R)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_R_anglo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use the shap.summary_plot with the subset of SHAP values and the corresponding X_train subset\n",
    "shap.summary_plot(shap_values_C, X_train, plot_size=(16,10), max_display=30, show=False)\n",
    "plt.title(f'SHAP Summary Classification Model Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use the shap.summary_plot with the subset of SHAP values and the corresponding X_train subset\n",
    "shap.summary_plot(shap_values_R, X_train_reg, plot_size=(16,10), max_display=30, show=False)\n",
    "plt.title(f'SHAP Summary Regression Model Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "feature_subset1 = ['Processed food in diet_A few times a month',\n",
    " 'Processed food in diet_A few times a week',\n",
    " 'Processed food in diet_Once a day',\n",
    " 'Processed food in diet_Rarely/Never',\n",
    " 'Processed food in diet_Several times a day']\n",
    "\n",
    "feature_subset2 = [\n",
    "'Frequency of doing exercise_Every day',\n",
    " 'Frequency of doing exercise_Few days a week',\n",
    " 'Frequency of doing exercise_Less than once a week',\n",
    " 'Frequency of doing exercise_Once a week',\n",
    " 'Frequency of doing exercise_Rarely/Never',\n",
    " #'Frequency of doing exercise_Several days a week',\n",
    " 'Frequency of doing exercise_Some days of the week']\n",
    "\n",
    "\n",
    "feature_subset3 = [\n",
    "\n",
    "#  'Sudden or premature death of a loved one',\n",
    "#  'Divorce/separation  or family breakup',\n",
    "#  'Extreme poverty leading to homelessness and/or hunger.',\n",
    "#  'Forced family control over major life decisions (e.g. marriage)',\n",
    " 'Prolonged sexual abuse| or severe sexual assault.',\n",
    "#  'Displacement from your home due to political| environmental or economic reasons',\n",
    " 'Loss of your job or livelihood leading to an inability to make ends meet.',\n",
    "#  'Cyberbullying or online abuse',\n",
    "#  'Threatening| coercive or controlling behavior by another person',\n",
    "#  'Caring for a child or partner with a major chronic disability or illness',\n",
    "#  'I did not experience any of the above',\n",
    "#  'Involvement or close witness to a war',\n",
    "#  'Life threatening or debilitating injury or illness.',\n",
    "#  'Suffered a loss in a major fire| flood| earthquake| or natural disaster',\n",
    "#  'None of the above AT',\n",
    "\n",
    "\n",
    "'Sudden or premature death of a parent or sibling',\n",
    "#  'Prolonged emotional or psychological abuse or neglect from parent/caregiver',\n",
    "#  'Prolonged physical abuse| or severe physical assault CT',\n",
    "#  'Physical violence in the home between family members',\n",
    "#  'Prolonged or sustained bullying in person from peers',\n",
    "#  'Parental Divorce or family breakup',\n",
    "#  'Lived with a parent/caregiver who was an alcoholic or who regularly used street drugs',\n",
    "#  'Threatening| coercive or controlling behavior by another person CT',\n",
    "#  'I did not experience any of the above during my childhood',\n",
    "#  'Suffered a loss in a major fire| flood| earthquake| or natural disaster CT',\n",
    "#  'Displacement from your home due to political| environmental or economic reasons CT',\n",
    "#  'Life threatening or debilitating injury or illness CT',\n",
    "#  'Forced family control over major life decisions CT',\n",
    "#  'None of the above CT'\n",
    " \n",
    " ]\n",
    "\n",
    "feature_subset4 = [\n",
    " 'Legal_Substance_Use',\n",
    " 'Interpersonal_Trauma',\n",
    " 'Life_Adversities'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_subset5 = [\n",
    "\n",
    "'Sudden or premature death of a loved one',\n",
    "'Divorce/separation  or family breakup',\n",
    "'Extreme poverty leading to homelessness and/or hunger.',\n",
    "'Forced family control over major life decisions (e.g. marriage)',\n",
    " 'Prolonged sexual abuse| or severe sexual assault.',\n",
    "'Displacement from your home due to political| environmental or economic reasons',\n",
    " 'Loss of your job or livelihood leading to an inability to make ends meet.',\n",
    "'Cyberbullying or online abuse',\n",
    "'Threatening| coercive or controlling behavior by another person',\n",
    "'Caring for a child or partner with a major chronic disability or illness',\n",
    "'I did not experience any of the above',\n",
    "'Involvement or close witness to a war',\n",
    "'Life threatening or debilitating injury or illness.',\n",
    "'Suffered a loss in a major fire| flood| earthquake| or natural disaster',\n",
    "'None of the above AT'\n",
    "\n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "feature_subset = feature_subset5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_names = X_train.columns  # Adjust if your feature names are stored differently\n",
    "desired_feature_indices = [i for i, feature in enumerate(feature_names) if feature in feature_subset]\n",
    "\n",
    "\n",
    "# This assumes shap_values_C.values is a structured array or similar where the first dimension is samples and the second is features\n",
    "subset_shap_values_array = shap_values_C.values[:, desired_feature_indices]\n",
    "\n",
    "\n",
    "\n",
    "X_train_subset = X_train[feature_subset]\n",
    "\n",
    "# Now use the shap.summary_plot with the subset of SHAP values and the corresponding X_train subset\n",
    "plt.title(f'SHAP Summary Classification Model Plot')\n",
    "shap.summary_plot(subset_shap_values_array, X_train_subset, plot_size=(12,6))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_names = X_train_reg.columns  # Adjust if your feature names are stored differently\n",
    "desired_feature_indices = [i for i, feature in enumerate(feature_names) if feature in feature_subset]\n",
    "\n",
    "\n",
    "# This assumes shap_values_C.values is a structured array or similar where the first dimension is samples and the second is features\n",
    "subset_shap_values_array_R = shap_values_R.values[:, desired_feature_indices]\n",
    "\n",
    "\n",
    "\n",
    "X_train_reg_subset = X_train_reg[feature_subset]\n",
    "\n",
    "# Now use the shap.summary_plot with the subset of SHAP values and the corresponding X_train subset\n",
    "plt.title(f'SHAP Summary Regression Model Plot')\n",
    "shap.summary_plot(subset_shap_values_array_R, X_train_reg_subset, plot_size=(12,6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.title(f'Mean|SHAP|) Classification Model Plot: United Kingdom (18-24)', fontsize=20)\n",
    "plt.title(f'Mean|SHAP|) Regression Model Plot: United Kingdom (45-64)', fontsize=20)\n",
    "shap.plots.bar(shap_values_R, max_display=50, show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df_org['Processed food in diet'])\n",
    "plt.show()\n",
    "\n",
    "df['Processed food in diet'].isna().sum()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  X_test / X_test_reg Change Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def test_model_with_modified_X_test(X_test, y_test, model, values_to_change, replacement_value, percentage_change):\n",
    "    # Make sure percentage_change is between 0 and 1\n",
    "    if not 0 <= percentage_change <= 1:\n",
    "        raise ValueError(\"percentage_change must be between 0 and 1.\")\n",
    "    \n",
    "    # Make sure replacement value is not in values to change\n",
    "    if replacement_value in values_to_change:\n",
    "        raise ValueError(\"replacement_value should not be in values_to_change.\")\n",
    "\n",
    "    # Make a copy of X_test to avoid changing the original data\n",
    "    X_test_modified = X_test.copy()\n",
    "\n",
    "    # Iterate over the specified values to be changed\n",
    "    for value in values_to_change:\n",
    "        # Find the indices where 'Processed food in diet' equals the current value\n",
    "        indices = X_test_modified[X_test_modified['Processed food in diet'] == value].index\n",
    "        # Calculate how many entries to replace\n",
    "        num_entries_to_replace = int(len(indices) * percentage_change)\n",
    "        # Randomly select entries to replace\n",
    "        indices_to_replace = np.random.choice(indices, size=num_entries_to_replace, replace=False)\n",
    "        # Replace the selected entries with the replacement value\n",
    "        X_test_modified.loc[indices_to_replace, 'Processed food in diet'] = replacement_value\n",
    "\n",
    "    # Predict with the modified X_test\n",
    "    predictions_proba = model.predict_proba(X_test_modified)\n",
    "    predictions = np.argmax(predictions_proba, axis=1)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions_proba[:, 1])\n",
    "    \n",
    "    # Calculate specificity (true negative rate)\n",
    "    tn, fp, _, _ = confusion_matrix(y_test, predictions).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Output results\n",
    "    print(\"xgb.XGBClassifier results with modified X_test:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(\"\\nPredicted Probabilities for each data point:\\n\", predictions_proba)\n",
    "\n",
    "    return accuracy, f1, precision, recall, specificity, auc, predictions_proba, X_test_modified\n",
    "\n",
    "# You can use this function as before, now with specificity included in the output.\n",
    "\n",
    "#****************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "def test_regressor_with_modified_X_test(X_test_reg, y_test_reg, model, values_to_change, replacement_value, percentage_change):\n",
    "    # Ensure percentage_change is between 0 and 1\n",
    "    if not 0 <= percentage_change <= 1:\n",
    "        raise ValueError(\"percentage_change must be between 0 and 1.\")\n",
    "    \n",
    "    # Ensure replacement value is not in values to change\n",
    "    if replacement_value in values_to_change:\n",
    "        raise ValueError(\"replacement_value should not be in values_to_change.\")\n",
    "\n",
    "    # Make a copy of X_test_reg to avoid changing the original data\n",
    "    X_test_modified = X_test_reg.copy()\n",
    "\n",
    "    # Iterate over the specified values to be changed\n",
    "    for value in values_to_change:\n",
    "        # Find the indices where the feature equals the current value\n",
    "        # Replace 'ProcessedFoodInDiet' with the actual feature name you are modifying\n",
    "        indices = X_test_modified[X_test_modified['Processed food in diet'] == value].index\n",
    "        num_entries_to_replace = int(len(indices) * percentage_change)\n",
    "        indices_to_replace = np.random.choice(indices, size=num_entries_to_replace, replace=False)\n",
    "        X_test_modified.loc[indices_to_replace, 'Processed food in diet'] = replacement_value\n",
    "\n",
    "    # Predict with the modified X_test_reg\n",
    "    predictions = model.predict(X_test_modified)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_test_reg, predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(y_test_reg, predictions)\n",
    "    mae = mean_absolute_error(y_test_reg, predictions)\n",
    "\n",
    "    # Output results\n",
    "    print(\"xgb.XGBRegressor results with modified X_test_reg:\")\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('Root Mean Squared Error:', rmse)\n",
    "    print('R-squared:', r2)\n",
    "\n",
    "    return mae, rmse, r2, predictions, X_test_modified\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     'A few times a week': 3,\n",
    "#     'Rarely/Never': 1,\n",
    "#     'A few times a month': 2,\n",
    "#     'Several times a day': 5,\n",
    "#     'Once a day': 4\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "values_to_change = [2, 3, 4, 5]\n",
    "replacement_value = 1\n",
    "percentage_change = 1.0\n",
    "\n",
    "test_regressor_with_modified_X_test(X_test_reg, y_test_reg, model_R, values_to_change, replacement_value, percentage_change)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     'A few times a week': 3,\n",
    "#     'Rarely/Never': 1,\n",
    "#     'A few times a month': 2,\n",
    "#     'Several times a day': 5,\n",
    "#     'Once a day': 4\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "values_to_change = [2, 3, 4, 5]\n",
    "replacement_value = 1\n",
    "percentage_change = 1.0\n",
    "\n",
    "test_model_with_modified_X_test(X_test, y_test, model_C, values_to_change, replacement_value, percentage_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#     'A few times a week': 3,\n",
    "#     'Rarely/Never': 1,\n",
    "#     'A few times a month': 2,\n",
    "#     'Several times a day': 5,\n",
    "#     'Once a day': 4\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "values_to_change = [2, 3, 4, 5]\n",
    "replacement_value = 1\n",
    "#test_model_with_modified_X_test(X_test, y_test, model_C, values_to_change, replacement_value, 0.1)\n",
    "\n",
    "\n",
    "accuracy, f1, precision, recall, specificity, auc, predictions_proba, X_test_modified = test_model_with_modified_X_test(X_test, y_test, model_C, values_to_change, replacement_value, 0.9)\n",
    "\n",
    "X_test_modified['Processed food in diet'].value_counts(normalize=True)\n",
    "\n",
    "plt.bar(X_test_modified['Processed food in diet'].value_counts().index, X_test_modified['Processed food in diet'].value_counts().values)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def plot_prediction_proba_distribution(predictions_proba, positive_class_column=1):\n",
    "#     # Extract the probabilities for the positive class\n",
    "#     positive_class_probs = predictions_proba[:, positive_class_column]\n",
    "\n",
    "#     # Create the plot\n",
    "#     sns.set(style=\"whitegrid\")\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.distplot(positive_class_probs, kde=True, color=\"blue\", hist=True, norm_hist=True)\n",
    "#     plt.title('Distribution of Predicted Probabilities for the Positive Class')\n",
    "#     plt.xlabel('Predicted Probability of Positive Class')\n",
    "#     plt.ylabel('Density')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def plot_prediction_proba_distribution(predictions_proba, positive_class_column=1):\n",
    "#     # Extract the probabilities for the positive class\n",
    "#     positive_class_probs = predictions_proba[:, positive_class_column]\n",
    "\n",
    "#     # Create the plot\n",
    "#     sns.set(style=\"whitegrid\")\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.distplot(positive_class_probs, kde=True, color=\"blue\", hist=False)\n",
    "#     plt.title('Distribution of Predicted Probabilities for the Positive Class')\n",
    "#     plt.xlabel('Predicted Probability of Positive Class')\n",
    "#     plt.ylabel('Density')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_combined_proba_distributions(proba_dict):\n",
    "    # Set up the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the KDE for each percentage_change's predicted probabilities\n",
    "    for per, predictions_proba in proba_dict.items():\n",
    "        positive_class_probs = predictions_proba[:, 1]  # Assuming positive class is the second column\n",
    "        sns.kdeplot(positive_class_probs, label=f'Percentage change: {per}', shade=False)\n",
    "    \n",
    "    plt.title('KDE of Predicted Probabilities for Different Percentage Changes')\n",
    "    plt.xlabel('Predicted Probability of Positive Class')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # predictions_proba is assumed to be obtained from the test_model_with_modified_X_test function\n",
    "# plot_prediction_proba_distribution(predictions_proba)\n",
    "\n",
    "\n",
    "# for per in [0.0, 1.0, 0.01]:\n",
    "#     accuracy, f1, precision, recall, specificity, auc, predictions_proba, X_test_modified = test_model_with_modified_X_test(\n",
    "#         X_test, y_test, model_C, values_to_change, replacement_value, per)\n",
    "#     plot_prediction_proba_distribution(predictions_proba)\n",
    "    \n",
    "\n",
    "#Usage:\n",
    "# Store the predicted probabilities in a dictionary with percentage_change as keys\n",
    "proba_dict = {}\n",
    "for per in [0.0, 0.5, 1.0]:\n",
    "    _, _, _, _, _, _, predictions_proba, _ = test_model_with_modified_X_test(\n",
    "        X_test, y_test, model_C, values_to_change, replacement_value, per)\n",
    "    proba_dict[per] = predictions_proba\n",
    "\n",
    "\n",
    "# Plot all KDEs on the same figure\n",
    "plot_combined_proba_distributions(proba_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_combined_prediction_distributions(prediction_dict):\n",
    "    # Set up the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the KDE for each percentage_change's predictions\n",
    "    for per, predictions in prediction_dict.items():\n",
    "        sns.kdeplot(predictions, label=f'Percentage change: {per}', shade=False)\n",
    "    \n",
    "    plt.title('KDE of Predicted Values for Different Percentage Changes')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "# Store the regression predictions in a dictionary with percentage_change as keys\n",
    "    \n",
    "#     'A few times a week': 3,\n",
    "#     'Rarely/Never': 1,\n",
    "#     'A few times a month': 2,\n",
    "#     'Several times a day': 5,\n",
    "#     'Once a day': 4\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "values_to_change = [2, 3, 4, 5]\n",
    "replacement_value = 1\n",
    "\n",
    "\n",
    "\n",
    "prediction_dict = {}\n",
    "for per in [0.0, 0.5, 1.0]:\n",
    "    mae, rmse, r2, predictions, X_test_modified = test_regressor_with_modified_X_test(\n",
    "        X_test_reg, y_test_reg, model_R, values_to_change, replacement_value, per)\n",
    "    prediction_dict[per] = predictions\n",
    "\n",
    "# Plot all KDEs on the same figure\n",
    "plot_combined_prediction_distributions(prediction_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_combined_prediction_frequencies_line(prediction_dict, bins=10):\n",
    "    # Set up the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate and plot the frequencies as lines for each percentage_change's predictions\n",
    "    for per, predictions in prediction_dict.items():\n",
    "        counts, bin_edges = np.histogram(predictions, bins=bins)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.plot(bin_centers, counts, label=f'Percentage change: {per}', marker='o', linestyle='-')\n",
    "    \n",
    "    plt.title('Frequency of Predicted Values for Different Percentage Changes (Line Plot)')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "values_to_change = [2, 3, 4, 5]\n",
    "replacement_value = 1\n",
    "\n",
    "prediction_dict = {}\n",
    "for per in [0.0, 0.2, 0.5, 0.8, 1.0]:\n",
    "    mae, rmse, r2, predictions, X_test_modified = test_regressor_with_modified_X_test(\n",
    "        X_test_reg, y_test_reg, model_R, values_to_change, replacement_value, per)\n",
    "    prediction_dict[per] = predictions\n",
    "\n",
    "# Plot frequency lines on the same figure\n",
    "plot_combined_prediction_frequencies_line(prediction_dict, bins=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_combined_prediction_differences_line(prediction_dict, bins=100):\n",
    "    # Set up the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Calculate the baseline histogram for per=0.0\n",
    "    baseline_counts, baseline_bin_edges = np.histogram(prediction_dict[0.0], bins=bins)\n",
    "    baseline_bin_centers = (baseline_bin_edges[:-1] + baseline_bin_edges[1:]) / 2\n",
    "    \n",
    "    # Plot differences for each percentage_change's predictions compared to the baseline\n",
    "    for per, predictions in prediction_dict.items():\n",
    "        if per == 0.0:  # Skip the baseline itself\n",
    "            continue\n",
    "        counts, bin_edges = np.histogram(predictions, bins=bins)\n",
    "        # Ensure same bins for all histograms by using the baseline bin edges\n",
    "        counts_aligned, _ = np.histogram(predictions, bins=baseline_bin_edges)\n",
    "        differences = counts_aligned - baseline_counts\n",
    "        plt.plot(baseline_bin_centers, differences, label=f'Percentage change: {per}', marker='o', linestyle='-')\n",
    "    \n",
    "    plt.title('Difference in Frequency of Predicted Values Compared to Baseline (per=0.0)')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Frequency Difference')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_prediction_differences_line(prediction_dict, bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_combined_prediction_differences_line(prediction_dict, bins=100):\n",
    "    # Set up the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate the baseline histogram for per=0.0\n",
    "    baseline_counts, baseline_bin_edges = np.histogram(prediction_dict[0.0], bins=bins)\n",
    "    baseline_bin_centers = (baseline_bin_edges[:-1] + baseline_bin_edges[1:]) / 2\n",
    "    \n",
    "    # Plot differences for each percentage_change's predictions compared to the baseline\n",
    "    for per, predictions in prediction_dict.items():\n",
    "        if per == 0.0:  # Skip the baseline itself\n",
    "            continue\n",
    "        counts, bin_edges = np.histogram(predictions, bins=bins)\n",
    "        # Ensure same bins for all histograms by using the baseline bin edges\n",
    "        counts_aligned, _ = np.histogram(predictions, bins=baseline_bin_edges)\n",
    "        differences = counts_aligned - baseline_counts\n",
    "        # Format the label to display percentages\n",
    "        percentage_label = f'{per*100}% change' if per != 0 else 'Baseline'\n",
    "        plt.plot(baseline_bin_centers, differences, label=percentage_label, marker='o', linestyle='-')\n",
    "    \n",
    "    plt.title('Difference in Frequency of Predicted Values Compared to Baseline for Anglosphere')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Frequency Difference')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_prediction_differences_line(prediction_dict, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_combined_prediction_differences_percentage_line(prediction_dict, bins=100, total_X_test=None):\n",
    "    # Check if total_X_test is provided\n",
    "    if total_X_test is None:\n",
    "        raise ValueError(\"Total number of observations in X_test must be provided.\")\n",
    "    \n",
    "    # Set up the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate the baseline histogram for per=0.0\n",
    "    baseline_counts, baseline_bin_edges = np.histogram(prediction_dict[0.0], bins=bins)\n",
    "    baseline_bin_centers = (baseline_bin_edges[:-1] + baseline_bin_edges[1:]) / 2\n",
    "    \n",
    "    # Plot differences for each percentage_change's predictions compared to the baseline\n",
    "    for per, predictions in prediction_dict.items():\n",
    "        if per == 0.0:  # Skip the baseline itself\n",
    "            continue\n",
    "        counts, bin_edges = np.histogram(predictions, bins=bins)\n",
    "        # Ensure same bins for all histograms by using the baseline bin edges\n",
    "        counts_aligned, _ = np.histogram(predictions, bins=baseline_bin_edges)\n",
    "        differences = counts_aligned - baseline_counts\n",
    "        # Convert differences to percentage of total X_test\n",
    "        differences_percentage = (differences / total_X_test) * 100\n",
    "        percentage_label = f'{per*100}% change' if per != 0 else 'Baseline'\n",
    "        plt.plot(baseline_bin_centers, differences_percentage, label=percentage_label, marker='o', linestyle='-')\n",
    "    \n",
    "    plt.title('Percentage Difference in Frequency of Predicted Values Compared to Baseline for Anglosphere')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Frequency Difference (%) of Total X_test')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_X_test = len(X_test)  # Assuming X_test is available and represents your test dataset\n",
    "plot_combined_prediction_differences_percentage_line(prediction_dict, bins=10, total_X_test=total_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot for % chanage from 0 to 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_and_plot_metrics(X_test, y_test, model, values_to_change, replacement_value, metrics_to_plot):\n",
    "    # Define the range of percentage changes to evaluate\n",
    "    percentage_changes = np.arange(0, 1.1, 0.1)\n",
    "    \n",
    "    # Initialize a dictionary to store the metrics for each percentage change\n",
    "    performance_metrics = {\n",
    "        'accuracy': [],\n",
    "        'f1': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'specificity': [],\n",
    "        'auc': []\n",
    "    }\n",
    "    \n",
    "    # Evaluate the model for each percentage change\n",
    "    for pc in percentage_changes:\n",
    "        # Use the provided test_model_with_modified_X_test function\n",
    "        acc, f1, prec, rec, spec, auc, _, _ = test_model_with_modified_X_test(\n",
    "            X_test, y_test, model, values_to_change, replacement_value, pc)\n",
    "        \n",
    "        # Store the metrics in the dictionary\n",
    "        performance_metrics['accuracy'].append(acc)\n",
    "        performance_metrics['f1'].append(f1)\n",
    "        performance_metrics['precision'].append(prec)\n",
    "        performance_metrics['recall'].append(rec)\n",
    "        performance_metrics['specificity'].append(spec)\n",
    "        performance_metrics['auc'].append(auc)\n",
    "\n",
    "    # Plot the specified metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for metric in metrics_to_plot:\n",
    "        plt.plot(percentage_changes, performance_metrics[metric], marker='o', label=f'{metric.title()}')\n",
    "\n",
    "    plt.title(f'Model Performance vs. Percentage Change')\n",
    "    plt.xlabel('Percentage Change')\n",
    "    plt.ylabel(metrics_to_plot)\n",
    "    plt.xticks(percentage_changes)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return performance_metrics\n",
    "\n",
    "# Example usage of the function:\n",
    "# metrics = evaluate_and_plot_metrics(X_test, y_test, model, [2, 3, 4, 5], 1, metrics_to_plot=['accuracy', 'recall', 'specificity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     'A few times a week': 3,\n",
    "#     'Rarely/Never': 1,\n",
    "#     'A few times a month': 2,\n",
    "#     'Several times a day': 5,\n",
    "#     'Once a day': 4\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "values_to_change = [2,3,4,5]\n",
    "replacement_value = 1\n",
    "\n",
    "evaluate_and_plot_metrics(X_test, y_test, model_C, values_to_change, replacement_value,  metrics_to_plot=['recall'])\n",
    "                                                                                                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import math\n",
    "\n",
    "def test_regressor_with_modified_X_test(X_test_reg, y_test_reg, model, percentage_change):\n",
    "    # Make sure percentage_change is between 0 and 1\n",
    "    if not 0 <= percentage_change <= 1:\n",
    "        raise ValueError(\"percentage_change must be between 0 and 1.\")\n",
    "    \n",
    "    # Make a copy of X_test_reg to avoid changing the original data\n",
    "    X_test_modified = X_test_reg.copy()\n",
    "\n",
    "    # Iterate over the unique values except 5\n",
    "    for value in [1, 2, 3, 4]:\n",
    "        # Find the indices where 'Processed food in diet' equals the current value\n",
    "        indices = X_test_modified[X_test_modified['Processed food in diet'] == value].index\n",
    "        # Calculate how many entries to replace\n",
    "        num_entries_to_replace = int(len(indices) * percentage_change)\n",
    "        # Randomly select entries to replace\n",
    "        indices_to_replace = np.random.choice(indices, size=num_entries_to_replace, replace=False)\n",
    "        # Replace the selected entries with 5\n",
    "        X_test_modified.loc[indices_to_replace, 'Processed food in diet'] = 5\n",
    "\n",
    "    # Predict with the modified X_test_reg\n",
    "    predictions = model.predict(X_test_modified)\n",
    "\n",
    "    # Evaluation\n",
    "    mse = mean_squared_error(y_test_reg, predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(y_test_reg, predictions)\n",
    "    mae = mean_absolute_error(y_test_reg, predictions)\n",
    "\n",
    "    # Output results\n",
    "    print(\"xgb.XGBRegressor results with modified X_test_reg:\")\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('Root Mean Squared Error:', rmse)\n",
    "    print('R-squared:', r2)\n",
    "    print(\"\\nPredicted Values for each data point:\\n\", predictions)\n",
    "\n",
    "    return mae, rmse, r2, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_regressor_with_modified_X_test(X_test_reg, y_test_reg, model_R, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, rmse, r2, predictions =test_regressor_with_modified_X_test(X_test_reg, y_test_reg, model_R, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_actual_vs_predicted(y_test_reg, predictions):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_reg, predictions, alpha=0.3)\n",
    "    plt.title('Actual vs. Predicted Values')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'k--', lw=4)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming y_test_reg and predictions are already defined\n",
    "# plot_actual_vs_predicted(y_test_reg, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_test_reg and predictions are already defined\n",
    "plot_actual_vs_predicted(y_test_reg, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def evaluate_and_plot_metrics(X_test, y_test, model, values_to_change, replacement_value, metric='accuracy'):\n",
    "    # Define the range of percentage changes to evaluate\n",
    "    percentage_changes = np.arange(0, 1.1, 0.1)\n",
    "    \n",
    "    # Initialize a dictionary to store the metrics for each percentage change\n",
    "    performance_metrics = {\n",
    "        'accuracy': [],\n",
    "        'f1': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'auc': []\n",
    "    }\n",
    "    \n",
    "    # Evaluate the model for each percentage change\n",
    "    for pc in percentage_changes:\n",
    "        # Use the provided test_model_with_modified_X_test function\n",
    "        acc, f1, prec, rec, auc, _, _ = test_model_with_modified_X_test(\n",
    "            X_test, y_test, model, values_to_change, replacement_value, pc)\n",
    "        \n",
    "        # Store the metrics in the dictionary\n",
    "        performance_metrics['accuracy'].append(acc)\n",
    "        performance_metrics['f1'].append(f1)\n",
    "        performance_metrics['precision'].append(prec)\n",
    "        performance_metrics['recall'].append(rec)\n",
    "        performance_metrics['auc'].append(auc)\n",
    "\n",
    "    # Plot the specified metric\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(percentage_changes, performance_metrics[metric], marker='o')\n",
    "    \n",
    "    plt.title(f'Model {metric.title()} vs. Percentage Change')\n",
    "    plt.xlabel('Percentage Change')\n",
    "    plt.ylabel(f'{metric.title()} Score')\n",
    "    plt.xticks(percentage_changes)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return performance_metrics\n",
    "\n",
    "# Example usage of the function:\n",
    "# metrics = evaluate_and_plot_metrics(X_test, y_test, model, [2, 3, 4, 5], 1, metric='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_histogram_with_averages(predictions, num_bins=20):\n",
    "    # Calculate histogram data\n",
    "    counts, bin_edges = np.histogram(predictions, bins=num_bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Calculate the average prediction per bin\n",
    "    averages = [predictions[(predictions >= bin_edges[i]) & (predictions < bin_edges[i+1])].mean() for i in range(num_bins)]\n",
    "    \n",
    "    # Plot the histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(predictions, bins=num_bins, alpha=0.5, label='Frequency')\n",
    "    \n",
    "    # Plot the average line per bin\n",
    "    plt.scatter(bin_centers, averages, color='red', label='Bin Averages')\n",
    "    \n",
    "    # Add a line representing the overall average\n",
    "    overall_avg = np.mean(predictions)\n",
    "    plt.axhline(overall_avg, color='green', linestyle='dashed', linewidth=2, label=f'Overall Average: {overall_avg:.2f}')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Histogram of Predicted Values with Bin Averages')\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'predictions' is already defined\n",
    "plot_histogram_with_averages(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_line_graph_of_bin_averages(predictions, num_bins=10):\n",
    "    # Calculate histogram data to determine bin edges\n",
    "    _, bin_edges = np.histogram(predictions, bins=num_bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Calculate the average prediction per bin\n",
    "    bin_averages = [np.mean(predictions[(predictions >= bin_edges[i]) & (predictions < bin_edges[i+1])]) for i in range(len(bin_centers))]\n",
    "\n",
    "    # Plot the line graph of averages\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(bin_centers, bin_averages, 'o-', color='red', label='Average Predicted Value')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Average Predicted Values per Bin')\n",
    "    plt.xlabel('Predicted Value Bins')\n",
    "    plt.ylabel('Average Predicted Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming 'predictions' is already defined\n",
    "plot_line_graph_of_bin_averages(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_prepare_df(df, countries=None, age_groups=None, drop_list=None):\n",
    "    \"\"\"\n",
    "    Segments the DataFrame based on specific countries and age groups, and then drops specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to be processed.\n",
    "    - countries: List of countries for segmentation. If None, no country-based segmentation is applied.\n",
    "    - age_groups: List of age groups for segmentation. If None, no age-based segmentation is applied.\n",
    "    - drop_list: List of columns to be dropped from the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame that has been segmented and had specified columns dropped.\n",
    "    \"\"\"\n",
    "    # Segment DataFrame based on provided countries and age groups\n",
    "    segmented_df = df.copy()\n",
    "    if countries is not None:\n",
    "        segmented_df = segmented_df[segmented_df['Country'].isin(countries)]\n",
    "    if age_groups is not None:\n",
    "        segmented_df = segmented_df[segmented_df['Age'].isin(age_groups)]\n",
    "    \n",
    "    # Drop specified columns if drop_list is provided\n",
    "    if drop_list is not None:\n",
    "        segmented_df = segmented_df.drop(columns=drop_list)\n",
    "    \n",
    "    return segmented_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your initial dataframe\n",
    "countries = ['United States', 'United Kingdom', 'Canada', 'Australia', 'New Zealand', 'Ireland', 'South Africa']\n",
    "age_groups = ['18-24', '21-24', '18', '19', '20']\n",
    "drop_list = [\n",
    "    'Country',\n",
    "    'Smartphone ownership',\n",
    "    'Friends/classmates smartphone ownership',\n",
    "    'Age of smartphone usage during school hours',\n",
    "    'Smartphone usage during class hours',\n",
    "    'Smartphone usage during break'\n",
    "]\n",
    "\n",
    "# Segment and prepare the DataFrame\n",
    "prepared_df = segment_and_prepare_df(df, countries=countries, age_groups=age_groups, drop_list=drop_list)\n",
    "\n",
    "# Continue with further data preparation steps as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, drop_list, target_classification, target_regression):\n",
    "    \"\"\"Prepares the dataset by dropping specified columns, encoding categorical features, and splitting into features and targets.\"\"\"\n",
    "    df_filtered = drop_columns(df, drop_list)\n",
    "    categorical_features = df_filtered.select_dtypes(include=['object']).columns\n",
    "    df_filtered = encode_features(df_filtered, categorical_features)\n",
    "    \n",
    "    X = df_filtered.drop(columns=[target_classification, target_regression], axis=1)\n",
    "    y_classification = df_filtered[target_classification]\n",
    "    y_regression = df_filtered[target_regression]\n",
    "    \n",
    "    return X, y_classification, y_regression\n",
    "\n",
    "def split_dataset(X, y_classification, y_regression, test_size=0.3, random_state=42):\n",
    "    \"\"\"Splits the dataset into training and testing sets for both classification and regression tasks.\"\"\"\n",
    "    X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=test_size, random_state=random_state)\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train_class, y_test_class, X_train_reg, X_test_reg, y_train_reg, y_test_reg\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use these functions\n",
    "drop_list = [\n",
    "    'Country',\n",
    "    'Smartphone ownership',\n",
    "    'Friends/classmates smartphone ownership',\n",
    "    'Age of smartphone usage during school hours',\n",
    "    'Smartphone usage during class hours',\n",
    "    'Smartphone usage during break'\n",
    "]\n",
    "\n",
    "# Assuming df is your initial dataframe\n",
    "X, y_classification, y_regression = prepare_dataset(df, drop_list, 'MHQ_Sign', 'Overall MHQ')\n",
    "X_train, X_test, y_train_class, y_test_class, X_train_reg, X_test_reg, y_train_reg, y_test_reg = split_dataset(X, y_classification, y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_processed = df[set10].copy()\n",
    "\n",
    "df_processed.loc[:, 'Processed food in diet'] = df_processed['Processed food in diet'].replace({\n",
    "            'Rarely/never': 'Rarely/Never',\n",
    "            'A few times in a day': 'Several times a day',\n",
    "            'Several days a week': 'A few times a week',\n",
    "            'Many times in a day': 'Several times a day',\n",
    "            'At least once a day': 'Several times a day',\n",
    "            'None of the above': 'None of the above SU'\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filter the encoded dataframe\n",
    "anglosphere = ['United States', 'United Kingdom', 'Canada', 'Australia', 'New Zealand', 'Ireland']\n",
    "\n",
    "\n",
    "kraj = ['United States']\n",
    "kraj2 =['India']\n",
    "        \n",
    "\n",
    "wiek = ['18-24', '21-24', '18', '19', '20']\n",
    "wiek2 = ['55-64','45-54']\n",
    "\n",
    "\n",
    "#df_filtered = df_processed[(df_processed['Country'].isin(kraj)) & (df_processed['Age'].isin(wiek))]\n",
    "\n",
    "\n",
    "#(2)\n",
    "#df_filtered = df_processed[(df_processed['Country'].isin(anglosphere))]\n",
    "\n",
    "#(1)\n",
    "#df_filtered = df_processed.copy()\n",
    "\n",
    "\n",
    "#(3) \n",
    "#df_filtered = df_processed[(df_processed['Country'].isin(kraj))]\n",
    "\n",
    "\n",
    "#4 & 5#\n",
    "#df_filtered = df_processed[(df_processed['Country'].isin(kraj)) & (df_processed['Age'].isin(wiek))]\n",
    "\n",
    "\n",
    "\n",
    "#df_filtered = df_processed[(df_processed['Age'].isin(wiek2))]\n",
    "\n",
    "\n",
    "\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
